{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89920e92",
   "metadata": {},
   "source": [
    "# Interpreting the input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915db7a",
   "metadata": {},
   "source": [
    "In the given dataset we observed that some cells had no value. For the sales file, we saw that for each pair of SKU and geoCluster there was usually at least a day with a\n",
    "value for the sale and price cell. We assumed the client sold a certain product that specific day and had the intention of selling it for the next days (where no price and sales are \n",
    "registered).\n",
    "We have not deleted those entries because we considered that they provided important information.\\\n",
    "On how we filled those null values, we though a good aproximation was the mean of the values that were actually in the table for that combination of SKU and geoClient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91586cb7",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa695a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0aa350",
   "metadata": {},
   "source": [
    "# Reading input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf303a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo_params = pd.read_csv('geo_params.csv')\n",
    "df_sales = pd.read_csv('sales.csv')\n",
    "df_sku = pd.read_csv('sku.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fdca0",
   "metadata": {},
   "source": [
    "# Cleaning datasets\n",
    "\n",
    "## Sales dataset\n",
    "\n",
    "### Removed unnecessary columns\n",
    "- ID\n",
    "\n",
    "### Null values treatment\n",
    "For each **SKU** and **geoCluster**, rows that have null values on **price** and **sales** have assigned the mean values of the rows that contain those values.\n",
    "\n",
    "## SKU dataset\n",
    "\n",
    "### Removed unnecessary columns\n",
    "- lagerUnitQuantity, Units, Type, trademark, brandId\n",
    "\n",
    "### Null values treatment\n",
    "- **Category**: as the **Group** of all the null values is **Youghurt**, they are assigned to the **Youghurts** category.\n",
    "- **countryOfOrigin**: null values have a new symbolic value of **0.0** assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914ce520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales.drop(labels='ID', axis=1)\n",
    "df_grouped = df_sales.groupby(['SKU', 'geoCluster'])\n",
    "df_means = df_grouped.mean()\n",
    "for index, row in df_means.iterrows():\n",
    "    df_filter = df_sales[(df_sales['SKU'] == index[0]) & (df_sales['geoCluster'] == index[1])]\n",
    "    df_sales.loc[(df_sales['SKU'] == index[0]) & (df_sales['geoCluster'] == index[1]), 'price'] = df_filter['price'].fillna(row['price'])\n",
    "    df_sales.loc[(df_sales['SKU'] == index[0]) & (df_sales['geoCluster'] == index[1]), 'sales'] = df_filter['sales'].fillna(row['sales'])\n",
    "\n",
    "df_sku = df_sku.drop(labels=['lagerUnitQuantity', 'Units', 'Type', 'trademark', 'brandId'], axis=1)\n",
    "df_sku['Category'] = df_sku['Category'].fillna('Yoghurts')\n",
    "df_sku['countryOfOrigin'] = df_sku['countryOfOrigin'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba0522",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006b66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.merge(df_sales, df_geo_params, on='geoCluster', how='inner')\n",
    "df_full = pd.merge(df_full, df_sku, on='SKU', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57526555",
   "metadata": {},
   "source": [
    "# Removing few remaining NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76798e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32997a29",
   "metadata": {},
   "source": [
    "# Changing date's strings to days since 1st Jan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ccec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDays(date_string): return (datetime.strptime(date_string, '%Y-%m-%d') - datetime(2020, 1, 1)).days\n",
    "df_full.loc[:, 'date'] = df_full['date'].apply(getDays)\n",
    "df_full.loc[:, 'countryOfOrigin'] = df_full['countryOfOrigin'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d588b2",
   "metadata": {},
   "source": [
    "# Normalizing the numerical features to their Z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770ec9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_df_full = df_full.loc[:, ['date', 'price', 'sales']].mean()\n",
    "std_df_full = df_full.loc[:, ['date', 'price', 'sales']].std()\n",
    "df_full.loc[:, ['date', 'price', 'sales']] = (df_full.loc[:, ['date', 'price', 'sales']] - mean_df_full) / std_df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37d180",
   "metadata": {},
   "source": [
    "# Creating training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0a429b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_training = df_full.sample(frac=0.8)\n",
    "df_testing = df_full.drop(df_training.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6067300",
   "metadata": {},
   "source": [
    "# Creating feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "fc_geoCluster = tf.feature_column.categorical_column_with_vocabulary_list(\"geoCluster\", df_training['geoCluster'].unique())\n",
    "fc_geoCluster = tf.feature_column.indicator_column(fc_geoCluster)\n",
    "feature_columns.append(fc_geoCluster)\n",
    "\n",
    "fc_SKU = tf.feature_column.categorical_column_with_vocabulary_list(\"SKU\", df_training['SKU'].unique())\n",
    "fc_SKU = tf.feature_column.indicator_column(fc_SKU)\n",
    "feature_columns.append(fc_SKU)\n",
    "\n",
    "resolution_in_Zs = 0.05\n",
    "\n",
    "fc_date_as_numeric_column = tf.feature_column.numeric_column(\"date\")\n",
    "fc_date_boundaries = list(np.arange(int(min(df_training['date'])), \n",
    "                                     int(max(df_training['date'])), \n",
    "                                     resolution_in_Zs))\n",
    "fc_date = tf.feature_column.bucketized_column(fc_date_as_numeric_column, fc_date_boundaries)\n",
    "\n",
    "fc_price = tf.feature_column.numeric_column(\"price\")\n",
    "feature_columns.append(fc_price)\n",
    "\n",
    "fc_cityId = tf.feature_column.categorical_column_with_vocabulary_list(\"cityId\", df_training['cityId'].unique())\n",
    "fc_cityId = tf.feature_column.indicator_column(fc_cityId)\n",
    "feature_columns.append(fc_cityId)\n",
    "\n",
    "fc_category = tf.feature_column.categorical_column_with_vocabulary_list(\"Category\", df_training['Category'].unique())\n",
    "fc_category = tf.feature_column.indicator_column(fc_category)\n",
    "feature_columns.append(fc_category)\n",
    "\n",
    "fc_countryOfOrigin = tf.feature_column.categorical_column_with_vocabulary_list(\"countryOfOrigin\", df_training['countryOfOrigin'].unique())\n",
    "fc_countryOfOrigin = tf.feature_column.indicator_column(fc_countryOfOrigin)\n",
    "feature_columns.append(fc_countryOfOrigin)\n",
    "\n",
    "fc_group = tf.feature_column.categorical_column_with_vocabulary_list(\"Group\", df_training['Group'].unique())\n",
    "fc_group = tf.feature_column.indicator_column(fc_group)\n",
    "feature_columns.append(fc_group)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d02da2",
   "metadata": {},
   "source": [
    "# Callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ff06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_early_stopping = 5\n",
    "patience_RLRL_callback = 2\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath = './models',\n",
    "  monitor=\"loss\",\n",
    "  verbose=1,\n",
    "  save_best_only=True,\n",
    "  mode=\"auto\",\n",
    "  save_freq=\"epoch\",\n",
    "  options=None,\n",
    "  )\n",
    "\n",
    "early_stoping_callback = tf.keras.callbacks.EarlyStopping(  \n",
    "  monitor=\"loss\",\n",
    "  patience=patience_early_stopping,\n",
    "  verbose=1,\n",
    "  mode=\"auto\",\n",
    ")\n",
    "\n",
    "RLRO_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor=\"loss\",\n",
    "  factor=0.025,\n",
    "  patience=patience_RLRL_callback,\n",
    "  verbose=1,\n",
    "  mode=\"auto\",\n",
    "  min_delta=0.0001,\n",
    "  cooldown=0,\n",
    "  min_lr=0,\n",
    ")\n",
    "\n",
    "model_callbacks = [model_checkpoint_callback, early_stoping_callback, RLRO_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379e3bc",
   "metadata": {},
   "source": [
    "# Building model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate, my_feature_layer):\n",
    "    \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    model.add(my_feature_layer)\n",
    "\n",
    "    # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
    "    # method once for each layer. We've specified the following arguments:\n",
    "    #   * units specifies the number of nodes in this layer.\n",
    "    #   * activation specifies the activation function (Rectified Linear Unit).\n",
    "    #   * name is just a string that can be useful when debugging.\n",
    "\n",
    "    # Define the first hidden layer with 20 nodes.   \n",
    "    model.add(tf.keras.layers.Dense(units=20, activation='relu',name='Hidden1'))\n",
    "    \n",
    "    # Define the second hidden layer with 12 nodes. \n",
    "    model.add(tf.keras.layers.Dense(units=12, activation='relu', name='Hidden2'))\n",
    "\n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=1, name='Output'))                              \n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model         \n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, label_name, batch_size=None):\n",
    "    \"\"\"Train the model by feeding it data.\"\"\"\n",
    "    # Split the dataset into features and label.\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(label_name))\n",
    "    \n",
    "    \n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, callbacks=model_callbacks) \n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # To track the progression of training, gather a snapshot\n",
    "    # of the model's mean squared error at each epoch. \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "    return epochs, mse\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ad456",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f43bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "batch_size = 15000\n",
    "label_name = \"sales\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, df_training, epochs, label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in df_testing.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46368423",
   "metadata": {},
   "source": [
    "# Predicting test sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ed9e160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoCluster</th>\n",
       "      <th>SKU</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>cityId</th>\n",
       "      <th>Category</th>\n",
       "      <th>countryOfOrigin</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>32485</td>\n",
       "      <td>1.562082</td>\n",
       "      <td>-0.409281</td>\n",
       "      <td>1</td>\n",
       "      <td>Banana</td>\n",
       "      <td>0</td>\n",
       "      <td>Tropical fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>32485</td>\n",
       "      <td>1.570105</td>\n",
       "      <td>-0.409281</td>\n",
       "      <td>1</td>\n",
       "      <td>Banana</td>\n",
       "      <td>0</td>\n",
       "      <td>Tropical fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>32485</td>\n",
       "      <td>1.578127</td>\n",
       "      <td>-0.409281</td>\n",
       "      <td>1</td>\n",
       "      <td>Banana</td>\n",
       "      <td>0</td>\n",
       "      <td>Tropical fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>32485</td>\n",
       "      <td>1.586150</td>\n",
       "      <td>-0.409281</td>\n",
       "      <td>1</td>\n",
       "      <td>Banana</td>\n",
       "      <td>0</td>\n",
       "      <td>Tropical fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>32485</td>\n",
       "      <td>1.594173</td>\n",
       "      <td>-0.409281</td>\n",
       "      <td>1</td>\n",
       "      <td>Banana</td>\n",
       "      <td>0</td>\n",
       "      <td>Tropical fruits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geoCluster    SKU      date     price  cityId Category  countryOfOrigin  \\\n",
       "0          21  32485  1.562082 -0.409281       1   Banana                0   \n",
       "1          21  32485  1.570105 -0.409281       1   Banana                0   \n",
       "2          21  32485  1.578127 -0.409281       1   Banana                0   \n",
       "3          21  32485  1.586150 -0.409281       1   Banana                0   \n",
       "4          21  32485  1.594173 -0.409281       1   Banana                0   \n",
       "\n",
       "             Group  \n",
       "0  Tropical fruits  \n",
       "1  Tropical fruits  \n",
       "2  Tropical fruits  \n",
       "3  Tropical fruits  \n",
       "4  Tropical fruits  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = pd.merge(df_test, df_geo_params, on='geoCluster', how='inner')\n",
    "df_test = pd.merge(df_test, df_sku, on='SKU', how='inner')\n",
    "df_test = df_test.drop(labels=['ID', 'sales'], axis=1)\n",
    "\n",
    "def getDays(date_string): return (datetime.strptime(date_string, '%Y-%m-%d') - datetime(2020, 1, 1)).days\n",
    "df_test.loc[:, 'date'] = df_test['date'].apply(getDays)\n",
    "df_test.loc[:, 'countryOfOrigin'] = df_test['countryOfOrigin'].astype('int')\n",
    "df_test.loc[:, 'countryOfOrigin'] = df_test['countryOfOrigin'].fillna(0.0)\n",
    "df_test.loc[:, 'Category'] = df_test['Category'].fillna('Yoghurts')\n",
    "df_test = df_test.rename(columns={'price_filled': 'price'})\n",
    "df_test.loc[:, ['date', 'price']] = (df_test.loc[:, ['date', 'price']] - mean_df_full) / std_df_full\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42a7b9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6336/6336 [==============================] - 51s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "test_features = {name:np.array(value) for name, value in df_test.items()}\n",
    "test_results = my_model.predict(test_features, verbose=1)\n",
    "test_results = test_results * std_df_full['sales'] + mean_df_full['sales']\n",
    "\n",
    "df_test_results = pd.read_csv('test.csv')\n",
    "df_test_results['sales'] = test_results\n",
    "df_test_results.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae2456",
   "metadata": {},
   "source": [
    "# Interpreting the model results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e568c8c",
   "metadata": {},
   "source": [
    "When we got the predictions for the sales, we were able to extract conclusions and ideas on how to improve them and the general benefit for our client.\n",
    "Some of our ideas are:\n",
    "\n",
    "\n",
    "*   If the sales' expectancy for a product is high, we see fit to make a market analysis, using our own predictor, on how the rise or reduction of the prize would affect the sales made. In any case, a reduction on the advertisement would be recommended as the product may not need it.\n",
    "\n",
    "*    On the contrary, if the sales' expectancy for a product is low, another market analysis would be advised but decreasing the price and analyze if by doing so, the sales would rise and the client wouldn't loose money by doing so. \n",
    "\n",
    "*    In any case, following the previous point, we see necessary an advertisement boost for that product. That way, we could increment its visibility. Another market analysis should be done to see if more advertisement would rise the sales.\n",
    "\n",
    "*    If we have two similar products that one is expected to be sold more than the other and both are related, such as two types of fruits or bakery, a good marketing idea would be to sell them together for a discounted price. That way we would be encouraging the users to buy them together, getting some economical benefit from both of them and not from just the popular one that they were already going to buy.\n",
    "\n",
    "*    Following the same idea as the previous point, if the client is not enthusiastic about making a pack with both products, another option would be putting them close, maybe one next to each other if they have a strong relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
